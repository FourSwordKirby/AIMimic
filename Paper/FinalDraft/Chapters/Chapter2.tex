\chapter{Related Work} % Main chapter title

\label{Chapter2} % For referencing the chapter elsewhere, use \ref{Chapter1} 

\section{Racing and Imitation}
One of the few documented instances of Imitation AI came from the racing game \textit{Forza Motorsport}. In this game, players could train "drivatars" to race just like themselves. This was implemented by having the AI learn how the player behaves on different segments of track and try to replicate that behavior when it encounters those sections. However, this imposed a restriction on the types of tracks that could be present in the game, as they had to be formed from the basic segment building blocks.

Researchers then expanded on this approach through the use of genetic algorithms \parencite{DrivingPlayerModeler}. In these kinds of algorithms, candidate solutions are continually evolved toward better solutions. These candidates have a set of parameters which are altered between each iteration of the process, and are then evaluated according to a fitness function. For the particular case of creating robust human-imitating racers, a fitness function made up of three different components was used. One focused on matching the player's progress on the track. Another focused on it matching the player's steering and a final one had it match the player's speed. The resulting AI did an alright job of mimicking some aspects of the respective players, as the AI imitating a slow, careful driver behaved in a significantly different way compared to the one that imitated a faster, reckless driver. However, closer inspection of the driving AI showed that the resulting behavior was not conceivably human. A later attempt which incorporated a focus on driving optimally into the fitness function also did not obtain convincing results. However, the study showed a clear trade-off between driving optimally and improving driver similarity. \parencite{MultiObjectiveMimic}

\section{Super Mario Bros.}
Researchers also developed several methods to mimic human behavior is in the space of 2D platformers, specifically a modified version of Super Mario Bros. \parencite{MarioImitation}. Novel methods tested in this space were Inverse Reinforcement Learning and Neuroevolution. 

The results of the Inverse Reinforcement Learning approach were discouraging, as the agent wasn't able to consistently handle situations that weren't often seen in the demonstration and was unable to match a human's ability to predict things that were not in the immediate detection area. In addition, the optimal policy obtained by IRL is deterministic, further reducing the human-like appearance of the AI \parencite{MarioImitation2}.

Neuroevolution produced much better results. In this method, a neural network was first trained to play Super Mario Bros. The state of the game was encoded into various genre specific variables that denoted the state of Mario and the distance of Mario to various obstacles. This was handed as input to the neural network, which was then expected to output the buttons that should be pressed in that situation. The resulting weights were then evolved and evaluated using a fitness function. The fitness function in this case was the distance between the AI and player's traces through the level. A key improvement made to suit this genre was to reset the AI's position if the distance between traces exceeded some threshold and apply a flat penalty. This is because an AI can easily get stuck in a 2D platformer, leading to a very bad final fitness score. The result was that the AI did the best job of mimicking human playstyles compared to many other algorithms. However, the agent achieved a lower score in the game compared to human players, showing that the agent had not really achieved a truly human-level of performance in the game.

\section{Fighting Games and Neural Nets}
Neuroevolutionary techniques have also been applied to fighting games. On a simple fighting game with 1 axis of movement, researchers found that evolutionary neural networks were able to quickly converge to an optimal playstyle \parencite{FightingAIComparison}. Additionally, Deep Reinforcement Learning has been able to create AI agents that can compete with top players in the popular commercial game Super Smash Bros. \parencite{SuperSmashBros}. However, the optimal AI were not ideal substitutes for playing against human opponents. For example, the Super Smash Bros. AI was specifically trained against only one kind of opponent, meaning that it was limited in the kinds of matchups it could perform well in. In addition, it exhibits obviously artificial traits such as impossible to perform rapid back and forth movements.

\section{Ghost AI}
With regards to creating AI that was specifically human-like, the most notable and widespread technique is Ghost AI. Researchers implemented a version of this algorithm on the commercial game Street Fighter. This AI initializes a histogram with the frequencies that target player performs actions in different situations uses those actions at the same frequency \parencite{GhostAI}. In the adaptive version, the actions also have an associated weight that updates based on the reward gained from performing them \parencite{GhostAI2}. These weights are then used to adjust the frequency that actions are selected.

To evaluate this AI, they recorded sessions of the player and their corresponding Ghost AI. The players were then asked to watch these both their own and the AI's recorded sessions and perform "phantom" inputs as if they were in the same situation. The recordings were then scored by the similarity of the recording inputs to the subject's "phantom" inputs. This method showed promising results, as the Ghost AI's similarity was able match around 75\% of the real recordings similarity. Players also expressed high qualitative satisfaction with their recordings, and the adaptive component allowed the AI to adjust itself to the strategies of the opponent. This approach has some notable pitfalls, as it does not account for specific player strategies that include varying timing. It also doesn't account for the opponent's state, such as how they are blocking, which factors into the decision making process for a real human player. 

\section{Data-driven Finite State Machines}
One final approach utilizes Data-Driven Finite State Machines. In this method, a multi-layered finite state machine is formed from the a log of a human demonstration. Specifically, the moves performed during the demonstration are annotated and used to designate the states of Finite State Machine. The transitions between these states are learned from the demonstrations. The state machine is then used to govern the AI's behavior during gameplay.

This approach has some clear limitations. For one, the annotation of moves is cumbersome and not well suited for a general purpose algorithm. Furthermore, the strategy that a player uses could be determined by an arbitrary number of in-game and out of game variables, which makes reducing player behavior to an FSM an daunting task. Lastly, this method was implemented on a 1D fighting game, which puts a huge limitation on the types of techniques that can be expressed by players.

\section{High Level Overview}
In this section we discussed several different existing methods for creating AI that mimic human-behavior. In domains where players progress on a path to an objective, such as racing games and 2D platformers, neuroevolution proves to be a strong strategy. However, there is a clear tradeoff between improving similarity and improving performance in these games, and even then these AI's have a hard time recovering from getting stuck. 

When looking specifically at fighting games, there is currently a lack of new developments. Though neural methods have proven effective at creating optimal agents in certain environments, they exhibit traits that prevent them from being suitable substitutes for human players. Other techniques such as Ghost AI have demonstrated an ability to express traits of human play, but are unable to capture things like a player's sense of timing. 